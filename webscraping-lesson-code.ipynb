{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2976bf24",
   "metadata": {},
   "source": [
    "# Web Scraping Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from selenium import webdrive\n",
    "import re\n",
    "import datetime\n",
    "import time, random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c6302",
   "metadata": {},
   "source": [
    "### Chapter 3 : [Intro BeautifulSoup](https://app.thisismetis.com/courses/144/pages/chapter-3-intro-to-beautifulsoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate specific tag elements\n",
    ".find() # first instance only\n",
    ".find_all() # all instances\n",
    "\n",
    "# Extract text\n",
    ".find().text # only work with find, for .find_all(), iterate over result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to extract text\n",
    "for item in soup.find_all('li'):\n",
    "    print(item.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to extract text\n",
    "\n",
    "[item.text for item in soup.find_all('li')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dcc6a",
   "metadata": {},
   "source": [
    "### Chapter 4 : [More BeautifulSoup Functionality](https://app.thisismetis.com/courses/144/pages/chapter-4-more-beautifulsoup-functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c484e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find anchor tags\n",
    "soup.find_all('a')\n",
    "\n",
    "# find element by attribute\n",
    "dickens = soup.find(id='dickens')\n",
    "\n",
    "# find element within element\n",
    "dickens.find_all('a')\n",
    "\n",
    "# chain methods\n",
    "soup.find(id='dickens').find_all('a')\n",
    "\n",
    "# extract attribute values\n",
    "soup.find('a')\n",
    "returns <a href=\"https...\">Pride and Prejudice</a>\n",
    "\n",
    "soup.find('a')['href']\n",
    "returns https..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to extract text\n",
    "\n",
    "for link in soup.find(id='dickens').find_all('a'):\n",
    "    print(link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to extract text\n",
    "\n",
    "[link['href'] for link in soup.find(id='dickens').find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ccc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find element by class attribute\n",
    "\n",
    "soup.find_all(class_='author')\n",
    "\n",
    "# find elements by multiple attributes\n",
    "\n",
    "soup.find_all(attrs={'class':'author', 'style':'font-size: 14px'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe671e9",
   "metadata": {},
   "source": [
    "### Chapter 5 : [Retriveing HTML with Requests](https://app.thisismetis.com/courses/144/pages/chapter-5-retrieving-html-with-requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests html as response \n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://www.thisismetis.com/blog'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "status = response.status_code\n",
    "\n",
    "if status == 200:\n",
    "  page = response.text\n",
    "  soup = bs(page)\n",
    "else:\n",
    "  print(f\"Oops! Received status code {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html from requests\n",
    "\n",
    "page_html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a29654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse with BS\n",
    "\n",
    "import bs4 import BeautifulSoup as bs\n",
    "\n",
    "soup = bs(page_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb759937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract html information\n",
    "\n",
    "soup.find('h1')\n",
    "soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee183a2",
   "metadata": {},
   "source": [
    "### Chapter 6 : [Locating by Text and DOM Position](https://app.thisismetis.com/courses/144/pages/chapter-6-locating-by-text-and-dom-position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa25602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate information by text\n",
    "\n",
    "soup.find(text='Welcome to Metis') # string must match exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate information with regular expresssion (regex)\n",
    "\n",
    "soup.find(text=re.compile('Welcome')) # returns string with welcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae45195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate information by position\n",
    "\n",
    "soup.find(text='Welcome to Metis').next # returns next element in code\n",
    "soup.find(text='Welcome to Metis').next.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate information by position-DOM\n",
    "\n",
    "welcome = soup.find(text='Welcome to Metis')\n",
    "welcome.parent\n",
    "welcome.parent.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf5d7b",
   "metadata": {},
   "source": [
    "### Chapter 6 : [Locating by Text and DOM Position Exercises](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/02_regression/regression_exercises/webscraping-chapter6-exercises.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_team_roles = [role.text for role in global_support.parent.parent.find_all('h5', class_='role')]\n",
    "\n",
    "print(global_team_roles)\n",
    "type(global_team_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_support = bs(requests.get(url).text).find(text='Global Support')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df7fa5",
   "metadata": {},
   "source": [
    "### Chapter 7 : [Data Preparation](https://app.thisismetis.com/courses/144/pages/chapter-7-data-preparation?module_item_id=5585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find data by text and position\n",
    "\n",
    "raw_mhi = \\\n",
    "    soup.find(text='Median household income'\n",
    "             ).next.next\n",
    "print(raw_mhi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e13974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract income string\n",
    "\n",
    "raw_mhi.split()[0] #returns first element of split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dollar sign and commas\n",
    "\n",
    "mhi_string = (raw_mhi.split()[0]\n",
    "                     .replace('$', '')\n",
    "                     .replace(',', ''))\n",
    "print(mhi_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b24090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to integer\n",
    "\n",
    "mhi = int(mhi_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021881fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom function to extract item, clean characters, and convert to int\n",
    "\n",
    "def get_mhi(soup):\n",
    "    try:\n",
    "        raw_mhi = soup.find(\n",
    "                    text = 'Median household income'\n",
    "                    ).next.next.text\n",
    "        mhi_string = (raw_mhi.split()[0]\n",
    "                             .replace('$', '')\n",
    "                             .replace(',', ''))\n",
    "        return int(mhi_string)\n",
    "    except:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef5692",
   "metadata": {},
   "source": [
    "### Chapter 7 : [Data Preparation Exercise](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/02_regression/regression_exercises/webscraping-chapter7-exercises.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d15e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = [post.text\n",
    "                .split('â€¢')[0]\n",
    "                .replace('By', ' ')\n",
    "                .strip()\n",
    "                for post in soup.find_all(class_= 'blog-post-details')[:5]]\n",
    "author_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501d1bb",
   "metadata": {},
   "source": [
    "### Chapter 9 : [Web Scraping Pitfalls](https://app.thisismetis.com/courses/144/pages/chapter-9-web-scraping-pitfalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pauses\n",
    "\n",
    "import time, random\n",
    "\n",
    "time.sleep(2) # pause two seconds\n",
    "time.sleep(1 + 2*random.random()) # pause randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try/except for error handling\n",
    "\n",
    "def get_mhi(soup):\n",
    "    try:\n",
    "        raw_mhi = soup.find(\n",
    "                    text = 'Median household income'\n",
    "                    ).next.next.text\n",
    "        mhi_string = (raw_mhi.split()[0]\n",
    "                             .replace('$', '')\n",
    "                             .replace(',', ''))\n",
    "        return int(mhi_string)\n",
    "    except:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize data with pickle\n",
    "\n",
    "import pickle\n",
    "\n",
    "# serialize data object\n",
    "\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    \n",
    "# read data object from pickle file\n",
    "\n",
    "with open('data.pickle', 'rb') as g:\n",
    "    new_data = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e445e4",
   "metadata": {},
   "source": [
    "### Chapter 10 : [Intro to Selenium](https://app.thisismetis.com/courses/144/pages/chapter-10-intro-to-selenium?module_item_id=5588)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e303521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdrive\n",
    "\n",
    "# provide path to chromedriver\n",
    "\n",
    "chromedriver = '/Applications/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = \\\n",
    "    chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "driver.get('https://wwww.thisismetis.com') # launches browser window\n",
    "\n",
    "print(driver.page_source[:100]) # extract html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_button = driver.find_element_by_xpath(\n",
    "    '//a[@id=\"application-link\"]')\n",
    "apply_button.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe6880",
   "metadata": {},
   "source": [
    "# [BeauifulSoup Notebook Lesson](http://localhost:8888/notebooks/Documents/GitHub/metis_dsml/02_regression/regression_exercises/web_scraping_beautifulsoup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b1d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to extract text of all list items\n",
    "\n",
    "todos=[element.text for element in soup.find_all('li')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7094e00",
   "metadata": {},
   "source": [
    "### Scraping Multiple Pages\n",
    "We'll also combine all previous steps into one helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9174f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_dict(link):\n",
    "    '''\n",
    "    From BoxOfficeMojo link stub, request movie html, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - title \n",
    "        - domestic gross\n",
    "        - runtime \n",
    "        - MPAA rating\n",
    "        - full release date\n",
    "    Return information as a dictionary.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.boxofficemojo.com'\n",
    "    \n",
    "    #Create full url to scrape\n",
    "    url = base_url + link\n",
    "    \n",
    "    #Request HTML and parse\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "    \n",
    "    headers = ['movie_title', 'domestic_total_gross',\n",
    "               'runtime_minutes', 'rating', 'release_date']\n",
    "    \n",
    "    #Get title\n",
    "    title_string = soup.find('title').text\n",
    "    title = title_string.split('-')[0].strip()\n",
    "\n",
    "    #Get domestic gross\n",
    "    raw_domestic_total_gross = (soup.find(class_='mojo-performance-summary-table')\n",
    "                                    .find_all('span', class_='money')[0]\n",
    "                                    .text\n",
    "                               )\n",
    "    domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "    #Get runtime\n",
    "    raw_runtime = get_movie_value(soup,'Running')\n",
    "    runtime = runtime_to_minutes(raw_runtime)\n",
    "    \n",
    "    #Get rating\n",
    "    rating = get_movie_value(soup,'MPAA')\n",
    "\n",
    "    #Get release date\n",
    "    raw_release_date = get_movie_value(soup,'Release Date').split('\\n')[0]\n",
    "    release_date = to_date(raw_release_date)\n",
    "    \n",
    "    #Create movie dictionary and return\n",
    "    movie_dict = dict(zip(headers, [title,\n",
    "                                domestic_total_gross,\n",
    "                                runtime,\n",
    "                                rating, \n",
    "                                release_date]))\n",
    "\n",
    "    return movie_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass each link stub to this function\n",
    "\n",
    "g_movies_page_info_list = []\n",
    "\n",
    "for link in g_movies.link_stub:\n",
    "    g_movies_page_info_list.append(get_movie_dict(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies_page_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b749c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_movies_page_info = pd.DataFrame(g_movies_page_info_list)  #convert list of dict to df\n",
    "g_movies_page_info.set_index('movie_title', inplace=True)\n",
    "\n",
    "g_movies_page_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c992f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: the rating is indeed missing from a few of these pages!  How could you fix that?)\n",
    "# We can now match this back up with the movie information collected \n",
    "# from the table by merging these dataframes.\n",
    "\n",
    "g_movies = g_movies.merge(g_movies_page_info, left_index=True, right_index=True)\n",
    "\n",
    "g_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493d19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fa894f",
   "metadata": {},
   "source": [
    "# [BeautifulSoup Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title\n",
    "# <title>The Dormouse's story</title>\n",
    "\n",
    "soup.title.name\n",
    "# u'title'\n",
    "\n",
    "soup.title.string\n",
    "# u'The Dormouse's story'\n",
    "\n",
    "soup.title.parent.name\n",
    "# u'head'\n",
    "\n",
    "soup.p\n",
    "# <p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "soup.p['class']\n",
    "# u'title'\n",
    "\n",
    "soup.a\n",
    "# <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
    "\n",
    "soup.find_all('a')\n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
    "\n",
    "soup.find(id=\"link3\")\n",
    "# <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b54c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One common task is extracting all the URLs found within a \n",
    "# pageâ€™s <a> tags:\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "# http://example.com/elsie\n",
    "# http://example.com/lacie\n",
    "# http://example.com/tillie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef02e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another common task is extracting all the text from a page:\n",
    "\n",
    "print(soup.get_text())\n",
    "# The Dormouse's story\n",
    "#\n",
    "# The Dormouse's story\n",
    "#\n",
    "# Once upon a time there were three little sisters; and their names were\n",
    "# Elsie,\n",
    "# Lacie and\n",
    "# Tillie;\n",
    "# and they lived at the bottom of a well.\n",
    "#\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
